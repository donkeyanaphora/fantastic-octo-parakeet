# fantastic-octo-parakeet

## Question
Do models flip-flop under the same conditions as people?

## Next Steps

1. **Prompts for rhetorical vs literal stances**
   - (Stance A) ‚Äî literal
   - (Stance A) ‚Äî rhetorical
   - [may not be relevant] (Stance ¬¨A) ‚Äî literal
   - [may not be relevant] (Stance ¬¨A) ‚Äî rhetorical
   - Need range of points of view on Stance A

2. **Steering directions** ‚Äî think about what directions to extract/apply

3. **FlipFlop datasets + relevant papers** ‚Äî gather and share ‚úîÔ∏è

4. **Publication venues** ‚Äî look into humanities/social sciences journals (where James Evans and Mina Lee publish)

5. **Basic activation steering resources** ‚Äî collect tutorials, repos, walkthroughs ‚úîÔ∏è

## Resources

### Human Belief Revision & Epistemology

- ‚≠ê [Other Minds (Austin, 1946)](https://web.stanford.edu/~paulsko/papers/Austin_OM.pdf) ‚Äî PDF
- ‚≠ê [Epistemic Vigilance (Sperber et al., 2010)](https://www.dan.sperber.fr/wp-content/uploads/2010_clement-et-al_epistemic-vigilance.pdf) ‚Äî PDF
- [Common Ground (Stalnaker, 2002)](https://semantics.uchicago.edu/kennedy/classes/f07/pragmatics/stalnaker02.pdf) ‚Äî PDF
- [John Langshaw Austin ‚Äî Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/entries/austin-jl/)
- [Willard Van Orman Quine ‚Äî Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/entries/quine/)

### Rhetoric & Persuasion

- ‚≠ê [Richard Rorty, *Contingency, Irony, and Solidarity* (1989)](https://sites.pitt.edu/~rbrandom/Courses/Antirepresentationalism%20(2020)/Texts/rorty-contingency-irony-and-solidarity-1989.pdf) ‚Äî PDF
- ‚≠ê [On the conversational persuasiveness of GPT-4 (Salvi et al., Nature 2025)](https://www.nature.com/articles/s41562-025-02194-6)
- ‚≠ê [Debating with More Persuasive LLMs Leads to More Truthful Answers](https://arxiv.org/abs/2402.06782)
- [Metaphor ‚Äî Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/entries/metaphor/)
- [Contradiction ‚Äî Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/entries/contradiction/)
- [Catachresis ‚Äî Wikipedia](https://en.wikipedia.org/wiki/Catachresis)
- [Exformation ‚Äî Wikiversity](https://en.wikiversity.org/wiki/Information)
- [Chiasmus ‚Äî Wikipedia](https://en.wikipedia.org/wiki/Chiasmus)

### Sycophancy ‚Äî Core Findings

- ‚≠ê [Towards Understanding Sycophancy in Language Models (Sharma et al., 2023)](https://arxiv.org/abs/2310.13548)
    - [Anthropic research page](https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models)
  
- ‚≠ê [Sycophancy to subterfuge: Investigating reward tampering in language models (Denison et al., 2024)](https://arxiv.org/abs/2406.10162)
    - [Anthropic research page](https://www.anthropic.com/research/reward-tampering)
### Stance Reversals & Challenge-Induced Flips

- ‚≠ê [Are You Sure? The FlipFlop Experiment (Laban et al., 2024)](https://arxiv.org/abs/2311.08596)
- [Ask Again, Then Fail (Xie et al., 2023/ACL 2024)](https://arxiv.org/abs/2310.02174)

### Social Conformity & Peer Pressure in Models

- [Disentangling the Drivers of LLM Social Conformity (Zhong et al., 2025)](https://arxiv.org/abs/2508.14918)
- [When Your AI Agent Succumbs to Peer-Pressure (Mehdizadeh & Hilbert, 2025)](https://arxiv.org/abs/2510.19107)
- [LLMs Can't Handle Peer Pressure / KAIROS (Song et al., 2025)](https://arxiv.org/abs/2508.18321)

### Metacognition & Dual-Process Thinking

- ‚≠ê [Metacognition for AI system safety (2022)](https://www.sciencedirect.com/science/article/pii/S0925753522000832)
- ‚≠ê [Fast, slow, and metacognitive thinking in AI (Nature, 2025)](https://www.nature.com/articles/s44387-025-00027-5)

### Calibration & Uncertainty

- [Can LLMs Express Their Uncertainty? (Xiong et al., ICLR 2024)](https://arxiv.org/abs/2306.13063)
- [Taming Overconfidence in LLMs: Reward Calibration in RLHF (Leng et al., ICLR 2025)](https://arxiv.org/abs/2410.09724)

### RLHF, Mode Collapse & Output Diversity

- ‚≠ê [Understanding the Effects of RLHF on LLM Generalisation and Diversity (Kirk et al., ICLR 2024)](https://openreview.net/pdf?id=PXD3FAVHJT)
- [Mysteries of Mode Collapse (LessWrong)](https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse)
- [One fish, two fish, but not the whole sea (Murthy et al., NAACL 2025)](https://aclanthology.org/2025.naacl-long.561.pdf)

### Preference Learning ‚Äî Background

- üîñ [InstructGPT (Ouyang et al., 2022)](https://arxiv.org/abs/2203.02155)
- üîñ [Direct Preference Optimization (Rafailov et al., 2023)](https://arxiv.org/abs/2305.18290)
- üîñ [Deep RL from Human Preferences (Christiano et al., 2017)](https://arxiv.org/abs/1706.03741)

### Steering & Representation Engineering

- ‚≠ê [The Linear Representation Hypothesis (Park et al., 2023)](https://arxiv.org/abs/2311.03658)
- ‚≠ê [Activation Engineering (Turner et al., 2023)](https://arxiv.org/abs/2308.10248)
- ‚≠ê [Representation Engineering (2025)](https://arxiv.org/abs/2502.17601)
- [Improving Instruction-Following through Activation Steering (Stolfo et al., ICLR 2025)](https://arxiv.org/abs/2410.12877)
- [Activation Scaling for Steering and Interpreting Language Models (EMNLP 2024)](https://aclanthology.org/2024.findings-emnlp.479.pdf)
- [Modulating Sycophancy via Activation Steering (LessWrong)](https://www.lesswrong.com/posts/raoeNarFYCxxyKAop/modulating-sycophancy-in-an-rlhf-model-via-activation)

### Targeted Sycophancy Mitigation

- [From Yes-Men to Truth-Tellers: Pinpoint Tuning (Chen et al., 2024)](https://arxiv.org/abs/2409.01658)

### Intelligence, Creativity & Diversity

- ‚≠ê [On the Measure of Intelligence (Chollet, 2019)](https://arxiv.org/abs/1911.01547)
- [Towards Benchmarking LLM Diversity & Creativity (Gwern)](https://gwern.net/creative-benchmark)

### Benchmarks & Evaluation Suites
- [TruthfulQA (Lin, Hilton, Evans, 2021)](https://arxiv.org/abs/2109.07958)
  - [Benchmark repo](https://github.com/sylinrl/TruthfulQA)
- [Sycophancy Eval](https://github.com/meg-tong/sycophancy-eval/blob/main/README.md)
- [SYCON-Bench (Hong et al., 2025)](https://arxiv.org/abs/2505.23840)
  - [Code/data repo](https://github.com/JiseungHong/SYCON-Bench)
- [GASLIGHTBENCH (Cui et al., 2025)](https://openreview.net/forum?id=0BYRYwGCbK)
- [MultiChallenge (2025)](https://arxiv.org/abs/2501.17399)
  - [ACL Anthology](https://aclanthology.org/2025.findings-acl.958/)

## Tools & Demos

- [Neuronpedia](https://www.neuronpedia.org)
- [Eiffel Tower Llama Demo](https://huggingface.co/spaces/dlouapre/eiffel-tower-llama)
- [Extracting Concepts from LLMs (HuggingFace blog)](https://huggingface.co/blog/m-ric/extracting-concepts-from-llms)
- [Llama3-Instruct code GitHub](https://github.com/donkeyanaphora/STEERING_EXPERIMENTS/blob/main/notebooks/llama_causal_probe.ipynb)

## Legend

- ‚≠ê = High priority / foundational
- üîñ = Background / reference